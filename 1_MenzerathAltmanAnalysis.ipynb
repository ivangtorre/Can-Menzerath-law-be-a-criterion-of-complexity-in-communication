{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from shutil import copyfile\n",
    "from nltk.tokenize import SyllableTokenizer\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "warnings.filterwarnings(\"error\")\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSP_hierarchy = [\n",
    "    \"aeiouyàáâäæãåāąèéêëēėęîïíīįìôöòóœøōõûüùúūůÿŷűőŵỳẁěýǫ\", # vowels.\n",
    "    \"ŭwł\", # approximants.\n",
    "    \"lrř\", # liquids.\n",
    "    \"mnñńŋň\", # nasals.\n",
    "    \"ßzvsfçćśŝĉĥhĵšžðđ\", # fricatives.\n",
    "    \"xjźżĝč\", # affricate.\n",
    "    \"bcdgtkpqþďť\", # occlusives.\n",
    "]\n",
    "\n",
    "\n",
    "def paralelized(text):\n",
    "    text = text.lower()          \n",
    "    try:                    \n",
    "        # Create vowels and consonants dictionary\n",
    "        keyconsonant = \"\" \n",
    "        for group in SSP_hierarchy[1::]:\n",
    "            keyconsonant += group\n",
    "        keyvowel = SSP_hierarchy[0]\n",
    "\n",
    "        consonant = dict.fromkeys(keyconsonant, 0)\n",
    "        vowels = dict.fromkeys(keyvowel, 0)\n",
    "\n",
    "        SSP = SyllableTokenizer(sonority_hierarchy = SSP_hierarchy)\n",
    "        tokenized_word = SSP.tokenize(text)\n",
    "        num_syllabes = len(tokenized_word)\n",
    "\n",
    "        #Length of words\n",
    "        sillenghts = []\n",
    "        for i in range(0, num_syllabes):\n",
    "            sillenghts.append(len(tokenized_word[i]))\n",
    "\n",
    "\n",
    "    except UserWarning:\n",
    "        pass         \n",
    "\n",
    "\n",
    "    for t in text:\n",
    "        if t in vowels: vowels[t] += 1\n",
    "        if t in consonant: consonant[t] += 1\n",
    "\n",
    "\n",
    "    total_vowels = sum(vowels.values())\n",
    "    total_consonant = sum(consonant.values())\n",
    "    if total_consonant + total_vowels == len(text):\n",
    "        return([text, total_consonant, total_vowels, num_syllabes, total_consonant + total_vowels,\n",
    "                (total_consonant + total_vowels) / num_syllabes, sillenghts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Menzerath-Altman Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusAnalysis:\n",
    "    def __init__(self, lang):\n",
    "        __slots__ = ['flag', 'consonants', 'vowels', 'words', 'syllabes', 'p_consonant', 'p_vowel',\n",
    "                     'p_silence', 'menzerath', 'df', 'syllabeslength', 'p_syllabeslength']\n",
    "        self.flag = True\n",
    "        self.consonants = 0\n",
    "        self.vowels = 0\n",
    "        self.words = 0\n",
    "        self.syllabes = 0\n",
    "        self.books = 0\n",
    "        \n",
    "        self.result_path = \"results/\" + lang + \"/\"\n",
    "        self.result_menzerathbyBook = self.result_path + \"MALbyBook_\" + lang + \".csv\"\n",
    "\n",
    "        with open(self.result_menzerathbyBook, \"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "        \n",
    "        self.p_consonant = 0.0\n",
    "        self.p_vowel = 0.0\n",
    "        self.p_silence = 0.0\n",
    "        self.menzerath = []\n",
    "        self.model = []\n",
    "        self.df = []\n",
    "        self.syllabeslength = {0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[], 10:[], 11:[], 12:[], 13:[], 14:[], 15:[], 16:[], 17:[], 18:[], 19:[], 20:[]}\n",
    "        self.p_syllabeslength = {0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[], 10:[], 11:[], 12:[], 13:[], 14:[], 15:[], 16:[], 17:[], 18:[], 19:[], 20:[]}\n",
    "\n",
    "\n",
    "    def save_results(self, out_path):\n",
    "        os.makedirs(os.path.dirname(self.result_path), exist_ok=True)\n",
    "\n",
    "        self.menzerath.to_csv(self.result_path + \"menzerath_altmann_\" + out_path + \".csv\")\n",
    "        chars_list = [('Total used books', self.books),\n",
    "                      ('Total used words', self.words),\n",
    "                      ('Total syllabes', self.syllabes),\n",
    "                      ('Total consonants', self.consonants),\n",
    "                      ('Total vowels', self.vowels),\n",
    "                      ('P(vowel)', self.p_vowel),\n",
    "                      ('P(consonant)', self.p_consonant),\n",
    "                      ('P(silence)', self.p_silence),\n",
    "                      ('Ratio syllabes/vowels', self.syllabes/self.vowels)\n",
    "                     ]\n",
    "        # Save Characteristics\n",
    "        pd.DataFrame(chars_list,\n",
    "                     columns=['feature', 'value']).to_csv(self.result_path + \"characteristics_\" + out_path+ \".csv\")\n",
    "\n",
    "        # Save HMM model\n",
    "        self.model.to_csv(self.result_path + \"HMM_menzerath_\" + out_path + \".csv\")\n",
    "        \n",
    "        # Save plot\n",
    "        self.plot(out_path)\n",
    "        \n",
    "        # Save Clusters size probabilities\n",
    "        pd.DataFrame.from_dict(data=self.p_syllabeslength, \n",
    "                               orient='index').to_csv(self.result_path + 'clustersizes_' + out_path + '.csv', header=False)\n",
    "\n",
    "\n",
    "    def add_corpus_analysis(self, token_list, language, method = \"SSP\"):\n",
    "        \"\"\"\n",
    "        Add corpus and analyse number of words, consonant and silences and syllabificate\n",
    "        \"\"\" \n",
    "        self.result_path = \"results/\" + language + \"/\"\n",
    "        words = [word for word in token_list if word.isalpha()]\n",
    "\n",
    "        with Pool(9) as p:\n",
    "            full_features_list = p.map(paralelized, words)\n",
    "\n",
    "            \n",
    "        # Update syllable length depending on position\n",
    "        features_list = []\n",
    "        for lista in full_features_list:\n",
    "            if lista is not None:\n",
    "                self.syllabeslength[lista[3]].append(lista[-1])\n",
    "                features_list.append(lista)\n",
    "        del full_features_list\n",
    "        \n",
    "        featuresDF = pd.DataFrame(features_list, columns=['token', 'consonant', 'vowels',  'syllabes', \n",
    "                                                          'letters', 'mean_syllabe_length', 'sillengths'])\n",
    "        \n",
    "        # Save a sample the first time of each language\n",
    "        if self.flag == True:\n",
    "            os.makedirs(os.path.dirname(self.result_path), exist_ok=True)\n",
    "            featuresDF.to_csv(self.result_path + \"SAMPLE.csv\", index=False)\n",
    "            self.flag = False\n",
    "        \n",
    "        \n",
    "        # Compute Vowels, Words, Consonant, Syllables\n",
    "        self.consonants += int(featuresDF['consonant'].sum())\n",
    "        self.vowels += int(featuresDF['vowels'].sum())\n",
    "        self.words += len(featuresDF)\n",
    "        self.syllabes += int(featuresDF['syllabes'].sum())\n",
    "        self.books += 1\n",
    "        \n",
    "        # Save syllables and meanSyllableLength \n",
    "        self.df.extend([tuple(x) for x in featuresDF[['syllabes', 'mean_syllabe_length']].to_numpy(dtype='float16')])\n",
    "        \n",
    "        # Menzerath for each book\n",
    "        MenzerathBook = featuresDF[['syllabes', 'mean_syllabe_length']].groupby(\"syllabes\").agg({'mean_syllabe_length':['mean']})\n",
    "        MenzerathBook = MenzerathBook['mean_syllabe_length']['mean'].to_list()\n",
    "        MenzerathBook.extend([0] * (20 - len(MenzerathBook)))\n",
    "        MenzerathBook = np.asarray(MenzerathBook)\n",
    "        \n",
    "        with open(self.result_menzerathbyBook, \"ab\") as f:\n",
    "            f.write(b\"\\n\")\n",
    "            np.savetxt(f, MenzerathBook, delimiter=\",\", newline=\" \", fmt='%1.4f')                                                                                   \n",
    "                                                                                                \n",
    "        \n",
    "    def menzerath_altmann(self):\n",
    "        \"\"\"\n",
    "        Compute BHMM model values\n",
    "        \"\"\"\n",
    "        self.p_consonant = self.consonants / (self.consonants + self.vowels + self.words)\n",
    "        self.p_vowel = self.vowels / (self.consonants + self.vowels + self.words)\n",
    "        self.p_silence = self.words / (self.consonants + self.vowels + self.words)\n",
    "        syllabeslength = self.syllabeslength\n",
    "        for key, _ in enumerate(self.syllabeslength):\n",
    "            if len(syllabeslength[key])>0:\n",
    "                self.p_syllabeslength[key] = np.sum(np.array(self.syllabeslength[key]), 0)/len(syllabeslength[key])\n",
    "        \n",
    "        # Compute Menzerath\n",
    "        self.menzerath = pd.DataFrame.from_records(self.df, columns=['syllabes', 'mean_syllabe_length']).groupby(\"syllabes\").agg({'mean_syllabe_length':['mean', 'std', 'count']})\n",
    "        self.HMM_model()\n",
    "\n",
    "        \n",
    "    def HMM_model(self):\n",
    "        \"\"\"\n",
    "        Compute BHMM model values\n",
    "        \"\"\"\n",
    "        p = self.p_consonant\n",
    "        model = []\n",
    "        for m in range(1, 20):\n",
    "            mean_length = (p)/(1-p)/m + 1/(1-p)\n",
    "            model.append((m, mean_length))\n",
    "\n",
    "        self.model = pd.DataFrame(model, columns=['feature', 'value'])\n",
    "\n",
    "        \n",
    "    def plot(self, out_path):\n",
    "        \"\"\"\n",
    "        Draft Plot\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(self.menzerath.index.values, self.menzerath.mean_syllabe_length[\"mean\"].values,\n",
    "                '--o', lw=2, ms=9, label=\"Corpus\")\n",
    "        ax.plot(self.model.feature, self.model.value, '--o', lw=2, ms=9, label=\"HMM model\")\n",
    "\n",
    "        ax.set_xlabel(\"word length [Syllables]\")\n",
    "        ax.set_ylabel(\"mean syllable length [Characters]\")\n",
    "        ax.legend()\n",
    "        fig.savefig(self.result_path + \"menzerath_altmann_\" + out_path + \".pdf\")\n",
    "        fig.savefig(\"results/menzerath_altmann_\" + out_path + \".pdf\")\n",
    "        plt.close('all')\n",
    "\n",
    "        \n",
    "def gutenberg_corpus(metadata, data_path, lang, sizelimit=None, method = \"SSP\"):   \n",
    "    analysis = CorpusAnalysis(lang)\n",
    "    lista_ficheros = []\n",
    "    metadata = metadata[metadata.type==\"Text\"]\n",
    "    for idname in metadata[metadata.language==\"['\"+ lang + \"']\"].id:\n",
    "        lista_ficheros.append(idname + \"_tokens.txt\")\n",
    "\n",
    "    if lang==\"en\":\n",
    "        random.seed(2)\n",
    "        random.shuffle(lista_ficheros)\n",
    "    \n",
    "    for index, file in enumerate(lista_ficheros[0:sizelimit]):\n",
    "        if index%10==0: print(\"Language:\" + lang +\" ,quedan: \" + str(len(lista_ficheros)-index))\n",
    "        huge_list = []\n",
    "        try:\n",
    "            with open(data_path+file, 'r') as input:\n",
    "                for line in input:\n",
    "                    huge_list.extend(line.split())\n",
    "            a = time.time()\n",
    "            if huge_list is not None: analysis.add_corpus_analysis(huge_list, lang, method)\n",
    "            if index%10==0: print(time.time() -a )\n",
    "                \n",
    "        except:\n",
    "            print(\"File \" + data_path + file + \" not found\")\n",
    "            pass\n",
    "        \n",
    "        if lang==\"en\":\n",
    "            print(analysis.books)\n",
    "\n",
    "        if analysis.books == 2500: # Max number of books\n",
    "            break\n",
    "        \n",
    "    analysis.menzerath_altmann()    \n",
    "    analysis.save_results(lang)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Languages and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "listaidiomas = [\"en\", \"fr\", \"fi\", \"de\", \"it\", \"nl\", \"es\", \"pt\", \"hu\", \"sv\", \"eo\", \"la\", \"da\",\n",
    "                 \"tl\", \"ca\", \"pl\", \"no\", \"cs\", \"cy\", \"is\", \"af\"]\n",
    "\n",
    "metadata = pd.read_csv(\"/path/from/metadata.csv\")\n",
    "data_path = \"/path/to/gutenberg/data/tokens/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Main analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in listaidiomas[::-1]:\n",
    "    print(\"Computing language: ****    \" + lang + \"  ****\")\n",
    "    gutenberg_corpus(metadata, data_path, lang, method = \"SSP\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
